{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"TODO List\\nLogic: \\n1. Display the path of the final output file in a messagebox or a label? \\n2. Error handling\\n    - Incorrect file selection (based on column names) - add elif to exception\\n    - file containing special characters that algorithm cannot process\\n3. Filepath and save path display on screen\\n4. Read from excel in pandas\\n5. Generating executable \\n6. find 'the' in POS tagging and ignore \\n\""
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''TODO List\n",
    "Logic: \n",
    "1. Display the path of the final output file in a messagebox or a label? \n",
    "2. Error handling\n",
    "    - Incorrect file selection (based on column names) - add elif to exception\n",
    "    - file containing special characters that algorithm cannot process\n",
    "3. Filepath and save path display on screen\n",
    "4. Read from excel in pandas\n",
    "5. Generating executable \n",
    "6. find 'the' in POS tagging and ignore \n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.7.14 |Anaconda, Inc.| (default, Nov  8 2017, 13:40:45) [MSC v.1500 64 bit (AMD64)]\n",
      "C:\\ProgramData\\Anaconda2\\python.exe\n",
      "<module 'sys' (built-in)>\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.version)\n",
    "print(sys.executable)\n",
    "print (sys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(sys)  \n",
    "sys.setdefaultencoding('utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MENU \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "import en\n",
    "\n",
    "#Tkinter libraries\n",
    "from Tkinter import *\n",
    "from Tkinter import Menu\n",
    "import tkFileDialog\n",
    "import Tkconstants\n",
    "import tkMessageBox\n",
    "\n",
    "#Training libraries\n",
    "from nltk.corpus import state_union\n",
    "from nltk.tokenize import PunktSentenceTokenizer\n",
    "\n",
    "#converting String to Pandas df\n",
    "import sys\n",
    "if sys.version_info[0] < 3: \n",
    "    from StringIO import StringIO\n",
    "else:\n",
    "    from io import StringIO\n",
    "    \n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training algorigthm\n",
    "train_text = state_union.raw(\"2005-GWBush.txt\")\n",
    "custom_sent_tokenizer = PunktSentenceTokenizer(train_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a function that will retrive only base verb forms\n",
    "is_verb = lambda pos: pos == 'VB' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Single 'should' - past tense\n",
    "def transform_stmt_single_shd_past(stmt): \n",
    "    words = nltk.word_tokenize(stmt)\n",
    "    veb_lst = [word for (word, pos) in nltk.pos_tag(words) if is_verb(pos)]\n",
    "   \n",
    "    if (stmt.find(\"should not\") > -1) and ('be' in veb_lst):\n",
    "        trnf_pst_stmt = stmt.replace(\"should not be\", \"was not\")\n",
    "        return trnf_pst_stmt\n",
    "    \n",
    "    elif (stmt.find(\"should not\") > -1):\n",
    "        trnf_pst_stmt = stmt.replace(\"should not\", \"did not\")\n",
    "        return trnf_pst_stmt\n",
    "\n",
    "    elif (stmt.find(\"should\") > -1) and (stmt.find(\"and\") > -1) and ('be' not in veb_lst) and (stmt.find(\"create\") > -1):\n",
    "        trnf_pst_stmt1 = stmt.replace(\"should \"+veb_lst[0], en.verb.past(veb_lst[0].lower()))\n",
    "        trnf_pst_stmt2 = trnf_pst_stmt1.replace(\"create\", \"created\")\n",
    "        return trnf_pst_stmt2\n",
    "        \n",
    "    elif (stmt.find(\"should\") > -1) and ('be' in veb_lst) and (stmt.find(\"details\") > stmt.find(\"field\")):\n",
    "        trnf_pst_stmt = stmt.replace(\"should be\", \"are\")\n",
    "        return trnf_pst_stmt\n",
    "    \n",
    "    elif (stmt.find(\"should\") > -1) and ('be' in veb_lst) and (stmt.find(\"sections\") > -1) and (stmt.find(\"screen\") > -1):\n",
    "        trnf_pst_stmt = stmt.replace(\"should be\", \"was\")\n",
    "        return trnf_pst_stmt\n",
    "    \n",
    "    elif (stmt.find(\"should\") > -1) and ('be' in veb_lst) and (stmt.find(\"details\") > -1):\n",
    "        trnf_pst_stmt = stmt.replace(\"should be\", \"were\")\n",
    "        return trnf_pst_stmt\n",
    "    \n",
    "    elif (stmt.find(\"should\") > -1) and ('be' in veb_lst) and (stmt.find(\"properties\") > -1):\n",
    "        trnf_pst_stmt = stmt.replace(\"should be\", \"were\")\n",
    "        return trnf_pst_stmt\n",
    "    \n",
    "    elif (stmt.find(\"should\") > -1) and ('be' in veb_lst) and (stmt.find(\"fields\") > -1):\n",
    "        trnf_pst_stmt = stmt.replace(\"should be\", \"were\")\n",
    "        return trnf_pst_stmt\n",
    "    \n",
    "    elif (stmt.find(\"should\") > -1) and ('be' in veb_lst) and (stmt.find(\"sections\") > -1):\n",
    "        trnf_pst_stmt = stmt.replace(\"should be\", \"were\")\n",
    "        return trnf_pst_stmt\n",
    "    \n",
    "    elif (stmt.find(\"should\") > -1) and ('be' in veb_lst):\n",
    "        trnf_pst_stmt = stmt.replace(\"should be\", \"was\")\n",
    "        return trnf_pst_stmt\n",
    "    \n",
    "    elif (stmt.find(\"should\") > -1) and ('be' not in veb_lst):\n",
    "        trnf_pst_stmt = stmt.replace(\"should \"+veb_lst[0], en.verb.past(veb_lst[0].lower()))\n",
    "        return trnf_pst_stmt\n",
    "    \n",
    "    else:\n",
    "        return stmt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Single 'should' - present tense\n",
    "def transform_stmt_single_shd_present(stmt): \n",
    "    words = nltk.word_tokenize(stmt)\n",
    "    veb_lst = [word for (word, pos) in nltk.pos_tag(words) if is_verb(pos)]\n",
    "    \n",
    "    if (stmt.find(\"should not\") > -1) and ('be' in veb_lst) and (stmt.find(\"fields\") > -1):\n",
    "        trnf_present_stmt = stmt.replace(\"should not be\", \"are not\")\n",
    "        return trnf_present_stmt        \n",
    "        \n",
    "    elif (stmt.find(\"should not\") > -1) and ('be' in veb_lst):\n",
    "        trnf_present_stmt = stmt.replace(\"should not be\", \"is not\")\n",
    "        return trnf_present_stmt\n",
    "    \n",
    "    elif (stmt.find(\"should not\") > -1):\n",
    "        trnf_present_stmt = stmt.replace(\"should not\", \"does not\")\n",
    "        return trnf_present_stmt\n",
    "\n",
    "    elif (stmt.find(\"should\") > -1) and (stmt.find(\"and\") > -1) and ('be' not in veb_lst) and (stmt.find(\"create\") > -1):\n",
    "        trnf_present_stmt1 = stmt.replace(\"should \"+veb_lst[0], en.verb.present(veb_lst[0].lower()))\n",
    "        trnf_present_stmt2 = trnf_present_stmt1.replace(\"create\", \"created\")\n",
    "        return trnf_present_stmt2\n",
    "    \n",
    "    elif (stmt.find(\"should\") > -1) and ('be' in veb_lst) and (stmt.find(\"details\") > stmt.find(\"field\")):\n",
    "        trnf_present_stmt = stmt.replace(\"should be\", \"is\")\n",
    "        return trnf_present_stmt   \n",
    "    \n",
    "    elif (stmt.find(\"should\") > -1) and ('be' in veb_lst) and (stmt.find(\"sections\") > -1) and (stmt.find(\"screen\") > -1):\n",
    "        trnf_present_stmt = stmt.replace(\"should be\", \"is\")\n",
    "        return trnf_present_stmt\n",
    "   \n",
    "    elif (stmt.find(\"should\") > -1) and ('be' in veb_lst) and (stmt.find(\"details\") > -1):\n",
    "        trnf_present_stmt = stmt.replace(\"should be\", \"are\")\n",
    "        return trnf_present_stmt\n",
    "    \n",
    "    elif (stmt.find(\"should\") > -1) and ('be' in veb_lst) and (stmt.find(\"properties\") > -1):\n",
    "        trnf_present_stmt = stmt.replace(\"should be\", \"are\")\n",
    "        return trnf_present_stmt\n",
    "    \n",
    "    elif (stmt.find(\"should\") > -1) and ('be' in veb_lst) and (stmt.find(\"fields\") > -1):\n",
    "        trnf_present_stmt = stmt.replace(\"should be\", \"are\")\n",
    "        return trnf_present_stmt\n",
    "    \n",
    "    elif (stmt.find(\"should\") > -1) and ('be' in veb_lst) and (stmt.find(\"sections\") > -1):\n",
    "        trnf_present_stmt = stmt.replace(\"should be\", \"are\")\n",
    "        return trnf_present_stmt\n",
    "    \n",
    "    elif (stmt.find(\"should\") > -1) and ('be' not in veb_lst) and (stmt.find(\"retrieve\") > -1):\n",
    "        trnf_present_stmt = stmt.replace(\"should retrieve\", \"retrieves\")\n",
    "        return trnf_present_stmt\n",
    "    \n",
    "    elif (stmt.find(\"should\") > -1) and ('be' in veb_lst):\n",
    "        trnf_present_stmt = stmt.replace(\"should be\", \"is\")\n",
    "        return trnf_present_stmt\n",
    "    \n",
    "    elif (stmt.find(\"should\") > -1) and ('be' not in veb_lst):\n",
    "        trnf_present_stmt = stmt.replace(\"should \"+veb_lst[0], en.verb.present(veb_lst[0].lower(), person=3, negate=False))\n",
    "        return trnf_present_stmt\n",
    "    \n",
    "    else:\n",
    "        return stmt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#multiple 'should' - present\n",
    "def transform_stmt_mul_shd_present(stmt):\n",
    "    temp_pst_collate = \"\"\n",
    "    splt_sentence = stmt.split(\"and\")\n",
    "    for stmt in splt_sentence:\n",
    "        temp_pst_collate = temp_pst_collate + 'and' + transform_stmt_single_shd_present(stmt)\n",
    "        #print(temp_pst_collate)\n",
    "    return temp_pst_collate[3:]      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#multiple 'should' - past \n",
    "def transform_stmt_mul_shd_past(stmt):\n",
    "    temp_pst_collate = \"\"\n",
    "    splt_sentence = stmt.split(\"and\")\n",
    "    for stmt in splt_sentence:\n",
    "        temp_pst_collate = temp_pst_collate + 'and' + transform_stmt_single_shd_past(stmt)\n",
    "        #print(temp_pst_collate)\n",
    "    return temp_pst_collate[3:]          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#process Expected result - convert to present tense\n",
    "def processExpRes_present(df):\n",
    "    #verb_lst = []\n",
    "    all_prsnt_stmt = []\n",
    "    temp_pst = \"\"\n",
    "\n",
    "    for test_case in list(df[\"test_Step_Expected_result\"]):\n",
    "        stmt_lst = custom_sent_tokenizer.tokenize(test_case)\n",
    "        try:\n",
    "            for stmt in stmt_lst:\n",
    "                words = nltk.word_tokenize(stmt)\n",
    "                if words.count(\"should\") > 1:\n",
    "                    temp_pst = temp_pst + '\\n' + transform_stmt_mul_shd_present(stmt)\n",
    "                else:\n",
    "                    temp_pst = temp_pst + '\\n' + transform_stmt_single_shd_present(stmt)\n",
    "                \n",
    "            all_prsnt_stmt.append(temp_pst[1:])\n",
    "            #print (temp_pst)\n",
    "            temp_pst = \"\"\n",
    "\n",
    "        except Exception as e:\n",
    "            all_prsnt_stmt.append(test_case)\n",
    "            print(str(e))\n",
    "    se = pd.Series(all_prsnt_stmt)\n",
    "    df['Conv_exptd_prsnt'] = se.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#process Expected result - convert to past tense\n",
    "def processExpRes_past(df):\n",
    "    #verb_lst = []\n",
    "    all_past_stmt = []\n",
    "    temp_pst = \"\"\n",
    "\n",
    "    for test_case in list(df[\"test_Step_Expected_result\"]):\n",
    "        stmt_lst = custom_sent_tokenizer.tokenize(test_case)\n",
    "        try:\n",
    "            for stmt in stmt_lst:\n",
    "                words = nltk.word_tokenize(stmt)\n",
    "                if words.count(\"should\") > 1:\n",
    "                    temp_pst = temp_pst + '\\n' + transform_stmt_mul_shd_past(stmt)\n",
    "                else:\n",
    "                    temp_pst = temp_pst + '\\n' + transform_stmt_single_shd_past(stmt)\n",
    "                \n",
    "            all_past_stmt.append(temp_pst[1:])\n",
    "            #print (temp_pst)\n",
    "            temp_pst = \"\"\n",
    "\n",
    "        except Exception as e:\n",
    "            all_past_stmt.append(test_case)\n",
    "            print(str(e))\n",
    "    se = pd.Series(all_past_stmt)\n",
    "    df['Conv_exptd_past'] = se.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#process actual result - trim User Info\n",
    "def processActRes(df):\n",
    "    trm_act_res = []\n",
    "    for test_case in list(df[\"test_step_Actual_result\"]):\n",
    "        try:\n",
    "            #print (test_case)\n",
    "            trm_act_res.append(test_case[:test_case.rfind(\"[\")])\n",
    "        except Exception as e:\n",
    "            trm_act_res.append(test_case)\n",
    "            print(str(e))   \n",
    "    ac = pd.Series(trm_act_res)\n",
    "    df['Converted_Actual'] = ac.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generatecompResult(df):\n",
    "    cmpexpactlst = []\n",
    "    resulttxt = ''\n",
    "    for index, row in df.iterrows():\n",
    "        #print(row[\"Converted_Expected\"] +\"----> \\n\"+  row[\"Converted_Actual\"])\n",
    "\n",
    "        x_prsnt = row[\"Conv_exptd_prsnt\"].replace(' ', '')\n",
    "        x1_prsnt = x_prsnt.replace('\\n', '') #replacing new lines\n",
    "        x2_prsnt = x1_prsnt.replace('\\r', '')#replacing returns\n",
    "        x3_prsnt = x2_prsnt.replace('\"','')#replacing \"\n",
    "        x4_prsnt = x3_prsnt.replace('.','')#replacing .\n",
    "        \n",
    "        x_past = row[\"Conv_exptd_past\"].replace(' ', '')\n",
    "        x1_past = x_past.replace('\\n', '') #replacing new lines\n",
    "        x2_past = x1_past.replace('\\r', '')#replacing returns\n",
    "        x3_past = x2_past.replace('\"','')#replacing \"\n",
    "        x4_past = x3_past.replace('.','')#replacing .\n",
    "\n",
    "        y = row[\"Converted_Actual\"].replace(' ', '')\n",
    "        y1 = y.replace('\\n', '') #replacing new lines\n",
    "        y2 = y1.replace('\\r', '') #replacing returnss\n",
    "        y3 = y2.replace('\"','')#replacing \"\n",
    "        y4 = y3.replace('.','')#replacing .\n",
    "\n",
    "        if (x4_prsnt == y4) or (x4_past == y4):\n",
    "            resulttxt = 'Approved'\n",
    "        elif (x4_prsnt in y4) or (x4_past in y4):\n",
    "            #y2.contains(x2):\n",
    "            resulttxt = 'Approved'\n",
    "        else:\n",
    "            resulttxt = 'Verify'\n",
    "        cmpexpactlst.append(resulttxt)\n",
    "    res = pd.Series(cmpexpactlst)\n",
    "    df[\"Compare_result\"] = res.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generateStepList(df):\n",
    "    keys,vals = df.sort_values('TEST_ID').values.T\n",
    "    ukeys, index = np.unique(keys, True)\n",
    "    arrays = np.split(vals, index[1:])\n",
    "    df2 = pd.DataFrame({'TEST_ID':ukeys, 'Steps to Verify':[list(a) for a in arrays]})\n",
    "    return df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generateSummary(df):\n",
    "    cnts = pd.DataFrame(df.TEST_ID.value_counts().reset_index())\n",
    "    cnts.columns = [\"TEST_ID\", \"Total Steps\"]\n",
    "    df1 = df.loc[df['Compare_result'] == \"Approved\"]\n",
    "    appcnts = pd.DataFrame(df1.TEST_ID.value_counts().reset_index())\n",
    "    appcnts.columns = [\"TEST_ID\", \"Approved\"]\n",
    "    df2 = df.loc[df['Compare_result'] == \"Verify\"]\n",
    "    vrfycnts = pd.DataFrame(df2.TEST_ID.value_counts().reset_index())\n",
    "    vrfycnts.columns = [\"TEST_ID\", \"Verify\"]\n",
    "    xxx = generateStepList(df2[['TEST_ID','test_step_name']])\n",
    "    df10 = cnts.merge(appcnts, on='TEST_ID', how='left').merge(vrfycnts, on='TEST_ID', how='left').merge(xxx, on='TEST_ID', how='left')\n",
    "    df11 = df10.fillna(0)\n",
    "    df11[['Approved','Verify']] = df11[['Approved','Verify']].astype(int)\n",
    "    df11['Action'] = df11.apply(lambda x: 'Approve' if x[\"Verify\"]==0 else 'Verify', axis=1)\n",
    "    final_sum_df = df11[[\"TEST_ID\", \"Action\", \"Total Steps\", \"Approved\", \"Verify\", \"Steps to Verify\"]]\n",
    "    return final_sum_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Close Main window\n",
    "def close_window(): \n",
    "    root.destroy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#main fn\n",
    "root = Tk()\n",
    "root.title(\"TEST RESULTS REVIEW\")\n",
    "\n",
    "#window size\n",
    "root.geometry('250x250')\n",
    "\n",
    "#window resizable = False\n",
    "root.resizable(False, False)\n",
    "\n",
    "def hello():\n",
    "      print \"hello!\"\n",
    "\n",
    "def generatefile():\n",
    "    try:\n",
    "        file = tkFileDialog.askopenfile(parent=root,mode='rb', title='Choose a file')\n",
    "        if file != None:\n",
    "            data = file.read()\n",
    "            \n",
    "            #convert file data from String to df\n",
    "            TESTDATA = StringIO(data)\n",
    "            df = pd.read_csv(TESTDATA, sep=\",\")\n",
    "            print (\"convert fn done\")\n",
    "            \n",
    "            #calling the process convert expected result- Present function here\n",
    "            processExpRes_present(df)\n",
    "            print (\"procesExpRes - present done\")\n",
    "            print (df.shape)\n",
    "            \n",
    "            #calling the process convert expected result- Past function here\n",
    "            processExpRes_past(df)\n",
    "            print (\"procesExpRes - Past done\")\n",
    "            print (df.shape)\n",
    "\n",
    "            #calling the process convert act result fucntion here\n",
    "            processActRes(df)\n",
    "            print (df.shape)\n",
    "\n",
    "            #calling generate comparison function here\n",
    "            generatecompResult(df)\n",
    "            print (df.shape)\n",
    "            \n",
    "            #generate summary stats\n",
    "            summaryStats_df = generateSummary(df)\n",
    "\n",
    "            time = datetime.datetime.now().strftime(\"%Y_%m_%d_%H%M%S\")\n",
    "            \n",
    "            #reload(sys)  \n",
    "            #sys.setdefaultencoding('utf-8')\n",
    "            with pd.ExcelWriter ('C:\\\\Users\\\\A6LBGZZ\\\\Bots\\\\./FinalOutput_{}.xlsx'.format(time)) as writer:\n",
    "                summaryStats_df.to_excel(writer, sheet_name = 'Summary', engine='xlsxwriter', encoding='utf-8', index=False)\n",
    "                df.to_excel(writer, sheet_name = 'Data', engine='xlsxwriter', encoding='utf-8', index=False)\n",
    "                writer.save()\n",
    "            #df.to_csv('C:\\\\Users\\\\A6LBGZZ\\\\Desktop\\\\./CFR_{}.csv'.format(time), encoding='utf-8')\n",
    "            #df.to_excel('./FinalOutput_{}.xlsx'.format(time), encoding='utf8')\n",
    "            tkMessageBox.showinfo(\"Results\", \"File saved\")\n",
    "\n",
    "            file.close()\n",
    "            print \"I got %d bytes from this file.\" % len(data)\n",
    "                        \n",
    "            #from GeneratingSummary import hello\n",
    "\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        if e ==\"'ascii' codec can't decode byte 0xe2 in position 163: ordinal not in range(128)\":\n",
    "            tkMessageBox.showinfo(\"Error\", \"Cannot write to xls\")\n",
    "\n",
    "menu = Menu(root)\n",
    "new_item = Menu(menu)\n",
    "\n",
    "filemenu = Menu(menu, tearoff=0)\n",
    "filemenu.add_command(label=\"Open\", command=hello)\n",
    "filemenu.add_command(label=\"Save\", command=hello)\n",
    "filemenu.add_separator()\n",
    "filemenu.add_command(label=\"Exit\", command=root.destroy)\n",
    "menu.add_cascade(label=\"File\", menu=filemenu)\n",
    "\n",
    "brwsLbl=Label(root)\n",
    "brwsLbl.grid(row=3, column=2)\n",
    "\n",
    "#prcsBtn=Button(root, text=\"BROWSE & PROCESS\", command=generatefile)\n",
    "#prcsBtn.grid(row=5, column=5)\n",
    "\n",
    "prcsBtn=Button(root, text=\"BROWSE & PROCESS\", command=generatefile)\n",
    "prcsBtn.place(bordermode = OUTSIDE,height=80, width=150)\n",
    "prcsBtn.place(relx=.5, rely=.2, anchor=\"c\") \n",
    "\n",
    "prcsBtn=Button(root, text=\"CLOSE\", command=close_window)\n",
    "prcsBtn.place(bordermode = INSIDE,height=80, width=150)\n",
    "prcsBtn.place(relx=.5, rely=0.6, anchor=\"c\") \n",
    "\n",
    "#exitBtn = Button(root, text = \"EXIT\", command = close_window)\n",
    "#exitBtn.grid(row=8, column=8)\n",
    "\n",
    "#table = tktable.Table(root, rows=10, cols=4)\n",
    "#table.pack(side=\"top\", fill=\"both\", expand=True)\n",
    "\n",
    "root.config(menu=menu)\n",
    "root.mainloop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = 'retrieve'\n",
    "print (en.verb.present(a.lower(), person=3, negate=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
