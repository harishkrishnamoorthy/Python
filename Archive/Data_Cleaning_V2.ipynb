{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "import en"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read input file"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "df = pd.read_csv('sams.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training the algorithm\n",
    "#To-do: Instead of speech train it on actual test case corpus\n",
    "from nltk.corpus import state_union\n",
    "from nltk.tokenize import PunktSentenceTokenizer\n",
    "train_text = state_union.raw(\"2005-GWBush.txt\")\n",
    "custom_sent_tokenizer = PunktSentenceTokenizer(train_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## fn: Rule engine for multiple 'should' statement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_stmt_mul_shd(stmt):\n",
    "    temp_pst_collate = \"\"\n",
    "    splt_sentence = stmt.split(\"and\")\n",
    "    for stmt in splt_sentence:\n",
    "        temp_pst_collate = temp_pst_collate + 'and' + transform_stmt_single_shd(stmt)\n",
    "        #print(temp_pst_collate)\n",
    "    return temp_pst_collate[3:]          "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## fn: Rule engine for single 'should' statement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_stmt_single_shd(stmt): \n",
    "    words = nltk.word_tokenize(stmt)\n",
    "    veb_lst = [word for (word, pos) in nltk.pos_tag(words) if is_verb(pos)]\n",
    "    \n",
    "    if (stmt.find(\"should not\") > -1) and ('be' in veb_lst):\n",
    "        trnf_pst_stmt = stmt.replace(\"should not be\", \"is not\")\n",
    "        return trnf_pst_stmt\n",
    "    \n",
    "    elif (stmt.find(\"should\") > -1) and ('be' in veb_lst) and (stmt.find(\"details\") > -1):\n",
    "        trnf_pst_stmt = stmt.replace(\"should be\", \"are\")\n",
    "        return trnf_pst_stmt\n",
    "    \n",
    "    elif (stmt.find(\"should\") > -1) and ('be' in veb_lst) and (stmt.find(\"properties\") > -1):\n",
    "        trnf_pst_stmt = stmt.replace(\"should be\", \"are\")\n",
    "        return trnf_pst_stmt\n",
    "    \n",
    "    elif (stmt.find(\"should\") > -1) and ('be' in veb_lst) and (stmt.find(\"fields\") > -1):\n",
    "        trnf_pst_stmt = stmt.replace(\"should be\", \"are\")\n",
    "        return trnf_pst_stmt\n",
    "    \n",
    "    elif (stmt.find(\"should\") > -1) and ('be' in veb_lst) and (stmt.find(\"sections\") > -1):\n",
    "        trnf_pst_stmt = stmt.replace(\"should be\", \"are\")\n",
    "        return trnf_pst_stmt\n",
    "         \n",
    "    elif (stmt.find(\"should\") > -1) and ('be' in veb_lst):\n",
    "        trnf_pst_stmt = stmt.replace(\"should be\", \"is\")\n",
    "        return trnf_pst_stmt\n",
    "        \n",
    "    elif (stmt.find(\"should\") > -1) and ('be' not in veb_lst):\n",
    "        trnf_pst_stmt = stmt.replace(\"should \"+veb_lst[0], en.verb.present(veb_lst[0].lower()))\n",
    "        return trnf_pst_stmt\n",
    "    \n",
    "    else:\n",
    "        return stmt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a function that will retrive only base verb forms\n",
    "is_verb = lambda pos: pos == 'VB' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "#verb_lst = []\n",
    "all_past_stmt = []\n",
    "temp_pst = \"\"\n",
    "\n",
    "for test_case in list(df[\"Expect_res\"]):\n",
    "    stmt_lst = custom_sent_tokenizer.tokenize(test_case)\n",
    "    try:\n",
    "        for stmt in stmt_lst:\n",
    "            words = nltk.word_tokenize(stmt)\n",
    "            if words.count(\"should\") > 1:\n",
    "                temp_pst = temp_pst + '\\n' + transform_stmt_mul_shd(stmt)\n",
    "                \n",
    "            else:\n",
    "                temp_pst = temp_pst + '\\n' + transform_stmt_single_shd(stmt)\n",
    "                \n",
    "        all_past_stmt.append(temp_pst[1:])\n",
    "        #print (temp_pst)\n",
    "        temp_pst = \"\"\n",
    "\n",
    "    except Exception as e:\n",
    "        all_past_stmt.append(test_case)\n",
    "        print(str(e))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert Actual Result - Cut [user: ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "trm_act_res = []\n",
    "for test_case in list(df[\"actualresult\"]):\n",
    "    try:\n",
    "        #print (test_case)\n",
    "        trm_act_res.append(test_case[:test_case.rfind(\"[\")])\n",
    "    except Exception as e:\n",
    "        trm_act_res.append(test_case)\n",
    "        #print(str(e))   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write \"Converted Expected\" & \"Converted Actual\" to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "se = pd.Series(all_past_stmt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "ac = pd.Series(trm_act_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Converted_Expected'] = se.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Converted_Actual'] = ac.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(931, 6)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#returns the dimesnsion of the data\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "931"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#returns number of lines in the list\n",
    "len(se)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(931, 6)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# display dimension of the df after adding out column\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df['out']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create new output file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(str.replace(' ', ''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'str2 = \"Testing the comparison between actual and expected for final results\"\\n(str1.replace(\\' \\', \\'\\')) == (str2.replace(\\' \\', \\'\\'))'"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''str2 = \"Testing the comparison between actual and expected for final results\"\n",
    "(str1.replace(' ', '')) == (str2.replace(' ', ''))'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "931"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare \"Converted expected\" & \"Converted Actual\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmpexpactlst = []\n",
    "resulttxt = ''\n",
    "for index, row in df.iterrows():\n",
    "    #print(row[\"Converted_Expected\"] +\"----> \\n\"+  row[\"Converted_Actual\"])\n",
    "    \n",
    "    x = row[\"Converted_Expected\"].replace(' ', '')\n",
    "    x1 = x.replace('\\n', '') #replacing new lines\n",
    "    x2 = x1.replace('\\r', '')#replacing returns\n",
    "    \n",
    "    y = row[\"Converted_Actual\"].replace(' ', '')\n",
    "    y1 = y.replace('\\n', '') #replacing new lines\n",
    "    y2 = y1.replace('\\r', '') #replacing returnss\n",
    "    \n",
    "    if x2 == y2:\n",
    "        resulttxt = 'Approved'\n",
    "    elif x2 in y2:\n",
    "        #y2.contains(x2):\n",
    "        resulttxt = 'Approved'\n",
    "    else:\n",
    "        resulttxt = 'Verify'\n",
    "    cmpexpactlst.append(resulttxt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write comparison results to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = pd.Series(cmpexpactlst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Compare_result\"] = res.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('Final_Output3.csv', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['System', 'web', 'the', 'web']\n",
      "['System', 'displays', 'the', 'web', 'page']\n",
      "Cosine: 0.73029674334\n"
     ]
    }
   ],
   "source": [
    "import re, math\n",
    "from collections import Counter\n",
    "\n",
    "WORD = re.compile(r'\\w+')\n",
    "\n",
    "def get_cosine(vec1, vec2):\n",
    "     intersection = set(vec1.keys()) & set(vec2.keys())\n",
    "     numerator = sum([vec1[x] * vec2[x] for x in intersection])\n",
    "\n",
    "     sum1 = sum([vec1[x]**2 for x in vec1.keys()])\n",
    "     sum2 = sum([vec2[x]**2 for x in vec2.keys()])\n",
    "     denominator = math.sqrt(sum1) * math.sqrt(sum2)\n",
    "\n",
    "     if not denominator:\n",
    "        return 0.0\n",
    "     else:\n",
    "        return float(numerator) / denominator\n",
    "\n",
    "def text_to_vector(text):\n",
    "     words = WORD.findall(text)\n",
    "     print (words)\n",
    "     return Counter(words)\n",
    "\n",
    "text1 = 'System web the web'\n",
    "text2 = 'System displays the web page'\n",
    "\n",
    "vector1 = text_to_vector(text1)\n",
    "vector2 = text_to_vector(text2)\n",
    "\n",
    "cosine = get_cosine(vector1, vector2)\n",
    "\n",
    "print 'Cosine:', cosine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
